\chapter{高效通信模型融合}

\section*{摘要}
我们考虑在通信轮数受到严重限制的情况下，学习一个联合模型的问题。我们讨论了最近关于模型融合的工作，这是联合学习的一个特例，只允许有一个通信回合。这种设置有一个独特的特点，即客户只要有一个预先训练好的模型，但没有数据就足够了。像GDPR这样的数据存储规定使这种设置很有吸引力，因为在FL开始之前更新本地模型后，数据可以立即被删除。然而，模型融合方法仅限于相对较浅的神经网络架构。我们讨论了适用于深度学习模型的模型融合的扩展，这些模型需要超过一轮的通信，但在通信预算方面仍然非常有效，即通信轮数和客户端与服务器之间交换的消息大小。我们考虑了同质和异质的客户端数据场景，包括由于数据中的偏差，在聚合数据上的训练是次优的场景。除了深度学习方法外，我们还涵盖了无监督的设置，如混合模型、主题模型和隐马尔科夫模型。

我们将模型融合的统计效率与假设的集中式方法进行比较，在这种方法中，一个拥有无限计算和存储能力的学习者简单地聚合所有客户的数据并以非联合的方式训练一个模型。正如我们将看到的，尽管模型融合方法通常与（假设的）集中式方法的收敛率相匹配，但它可能没有同样的效率。此外，当客户数据是异质的时候，集中式和联合式方法之间的这种差异会被放大。

\section{导言}
标准的联合学习算法，例如联合平均法[34]，在聚合来自客户端的模型参数时，依赖于简单的参数平均法。由于其简单性，这种方法与大多数模型和深度学习架构兼容。然而，它也有一些缺点。特别是，在许多应用领域，训练一个高性能模型所需的通信回合数通常是数百个。这种通信成本可能是令人望而却步的，特别是在通信轮次的开销很高的情况下。例如，在客户端是移动设备的应用中，一轮通信可能对应于每天与服务器的同步任务。在其他应用中，启动一轮通信可能需要人类的批准（例如，医院形成一个数据链）。

上述的挑战促使我们考虑只需要几轮通信的联合学习算法。我们回顾一下最近的模型融合[48, 49]技术，它只需要单轮通信。模型融合方法有一个额外的好处，即客户不需要存储数据，也就是说，他们只需要提供他们的本地模型，用于联合学习一个更强大的全局模型。数据存储受到GDPR[19]的监管，使得模型融合的这一特点具有实际的吸引力。正如我们将看到的，模型融合技术依赖于通过匈牙利算法[28]或Wasserstein barycenters[1, 42]的变体进行的双点匹配[49]。这些方法在平均参数时考虑到了模型组件（如神经元权重）之间的相似性，这使得它们能够在短短一个通信回合内产生良好的全局模型。缺点是，对于VGG架构等深度神经网络来说，相应的优化问题对准模型组件变得难以解决[41]。

为了处理深层神经网络，我们也考虑了[47]的层级匹配策略，它可以在固定的通信回合数（取决于神经网络的深度）内训练一个强大的联合模型。然而，这种方法需要客户端存储数据，以便像其他联合学习算法那样在本地执行模型更新。

最后，我们还探讨了模型融合的统计特性。特别是，我们将模型融合的方法与一个假设的集中式方法进行比较，在这个方法中，一个服务器聚集了所有来自客户端的数据，并在没有任何通信限制的情况下训练一个ML模型。这在联合学习的许多应用中是不现实的，但它是一个黄金标准，我们可以将模型融合的统计性能与之进行比较。理想情况下，我们希望模型融合能够与这种（假设的）集中式方法的统计效率相匹配。正如我们将看到的，这几乎是事实。

在这一章中，我们回顾了模型融合技术，并展示了它们在单轮通信中与较简单的神经网络架构的联合学习的适用性[49]；与混合模型、隐马尔科夫模型和主题模型的无监督学习[48, 50]。我们提出了用于后验融合的扩展[14]，以支持贝叶斯神经网络的联合学习[36]。我们回顾了[47]的方法，该方法适用于有限的通信预算下的深度神经网络的联合学习。最后，我们研究了模型融合的统计特性，最后我们讨论了开放的挑战和有希望的未来工作方向。

\section{模型的互换-不变结构}
许多机器学习模型可以用参数向量集而不是参数单向量来描述。例如，一个用于聚类的混合模型是由一组聚类中心点来描述的。集合中的中心点顺序的任何变化都会产生等效的聚类质量，即相同的数据可能性。在联合学习的背景下，现在很明显，为什么简单地对从不同客户处获得的两套聚类中心点进行平均化可能是有害的：即使在最简单的情况下，两个客户在同质数据集上拟合聚类模型并恢复相同的中心点，其解决方案中中心点的排序是任意的，元素平均化可能会导致糟糕的联合全球模型。本章中我们将介绍的其他突出的包络变量无监督模型的例子是主题模型和隐马尔科夫模型。

模型参数的互换不变性也存在于监督学习中，特别是神经网络中。考虑一个简单的具有L个隐藏单元的单隐藏层全连接神经网络
\begin{align}\label{eq:7.1}
	f(x) = \sigma (x W_{1}) W_{2}
\end{align}
其中$\sigma(\cdot)$是一个非线性的应用，$x \in \mathbb{R}^{D}, W_{1} \in \mathbb{R}^{D \times L}, W_{2} \in \mathbb{R}^{L \times K}$。$D$和$K$是输入和输出维度，我们省略偏置项，不失为一种通用的方法。让$W_{1, \cdot l}$表示$W_{1}$的第$l$列，$W_{2, l \cdot}$表示$W_{2}$的第$l$行，那么我们可以将（\ref{eq:7.1}）写成
\begin{align}\label{eq:7.2}
	f(x) = \sum_{l=1}^{L}W_{2, l \cdot} \sigma(<x, W_{1, \cdot l}>)
\end{align}
和是一个不变的操作，因此任何神经元的重新排序，即$W_{1}$的列和$W_{2}$的相应行，都会导致一个具有相同预测规则的神经网络。为了说明互换不变性，我们将（\ref{eq:7.1}）重写为
\begin{align}\label{eq:7.3}
	f(x) = \sigma (x W_{1} \prod) \prod^{T}W_{2}
\end{align}
其中$\prod$是$L!$个可能的互换矩阵之一。回顾一下，排列矩阵是一个正交矩阵，它在左边应用时作用于行，在右边应用时作用于列。假设$\left\{ W_{1}, W_{2} \right\}$是最佳权重，那么，根据公式（\ref{eq:7.3}），在两个同质数据集$X_{j}$和$X_{j}^{'}$上训练会产生两组权重$\left\{ W_{1}\prod_{j}, \prod_{j}^{T}W_{2} \right\}$和$\left\{ W_{1}\prod_{j^{'}}, \prod_{j^{'}}^{T}W_{2} \right\}$。这两组参数的天真平均是次优的，也就是说，$\prod_{j} \neq \prod_{j^{'}}$的概率很高，因此$\frac{1}{2}(W_{1}\prod_{j} + W_{1}\prod_{j^{'}}) \neq W_{1}\prod$为任何$\prod$。为了优化神经网络权重的平均化，我们首先应该撤销排列组合$(W_{1}\prod_{j}\prod_{j}^{T} + W_{1}\prod_{j^{'}}\prod_{j^{'}}^{T})/2 = W_{1}$。

超越排列组合的神经网络不变性
我们在本章将要介绍的模型融合技术是为了在进行融合时考虑到客户模型的排列组合不变性。然而，在神经网络的情况下，可能存在其他不变性。神经网络通常是巨大的超参数化，学习相应的权重是一个非凸的优化问题，可能有许多等价的（在训练损失的意义上）局部优化。对于一个有$L$个神经元的单隐层神经网络，如方程（\ref{eq:7.1}），由于包络不变性，对于任何解决方案，至少有$L$个等价的解决方案。有可能存在其他不变的（或其他相等的）解决方案。这是文献中一个尚未解决的问题。一个潜在的富有成效的观点是研究神经网络的损失景观--对这个问题还没有完整的理论认识；然而，已经取得了一些进展[15, 18, 21, 30]。考虑损失景观的角度来开发新的联合学习和模型融合算法是一个有趣的未来工作方向。

\subsection{匹配平均法的一般公式}
我们现在按照[47]的观点，将具有内在包络不变性的模型的参数平均化的想法正式化。我们继续以一个隐藏层的神经网络为例；然而，正如我们将在随后的章节中展示的那样，这个想法很容易被推广到其他模型。

让$w_{jl}$成为在第$j$个客户数据上学到的第$l$个神经元权重。我们认为$w_{jl}$是前面讨论过的$W_{1}$的第$l$列和$W_{2}$的第$l$行的连接。让$\theta$代表全局模型中（未知的）第$i$个神经元权重，$c(\cdot, \cdot)$是一个适当的相似性函数，例如平方欧氏距离。匹配的平均化优化问题如下：
\begin{align}\label{eq:7.4}
	\min_{\left\{ \pi_{i,l}^{j} \in \left\{ 0, 1 \right\} \right\}} \sum_{i=1}^{L}\sum_{j,l} \min_{\theta_{i}} \pi_{il}^{j}c(w_{jl}, \theta_{i})s.t. \sum_{i}\pi_{il}^{j} = 1 \forall j, l; \sum_{l} \pi_{il}^{j} = 1 \forall i, j
\end{align}
当相似度$c(\cdot, \cdot)$是一个平方的欧氏距离时，$\theta_{i}$的内部优化是微不足道的，也就是说，它是匹配的客户神经元权重$\theta_{i} = \frac{\sum_{j,l}\pi_{i,l}^{j}w_{jl}}{\sum_{j,l}\pi_{i,l}^{j}}$的平均值。匹配平均的名称是由于方程（7.4）与最大双点匹配问题的关系。这个优化问题也与Wasserstein barycenter[1]有关，[42]在其基于最优传输的模型融合方法中利用了这个问题。