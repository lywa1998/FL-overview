# 分离学习：分布式深度学习的一种资源节约型模型和数据并行方法

## 摘要

资源限制、工作量开销、缺乏信任和竞争阻碍了多个机构之间共享原始数据。这导致了用于训练最先进的深度学习模型的数据短缺。分裂学习是一种分布式机器学习的模型和数据并行方法，是克服这些问题的高度资源效率的解决方案。分离式学习通过对传统的深度学习模型架构进行划分，使网络中的一些层对客户是私有的，其余的在服务器上集中共享。这使得分布式机器学习模型的训练不需要任何原始数据的共享，同时减少任何客户端所需的计算或通信量。分离式学习的范式有几种变体，取决于手头正在考虑的具体问题。在本章中，我们将分享执行分割学习的理论、经验和实践方面，以及一些可以根据你选择的应用而选择的变体。

## 19.1 分离学习的介绍

联合学习[1]是一种数据并行的方法，其中数据是分布式的，而作为训练回合一部分的每个客户端都使用自己的本地数据训练完全相同的模型架构。在现实世界中，服务器可能是一个强大的计算资源，但最终却要进行一个相对简单的计算，即对每个客户端学习的权重进行加权平均。在现实世界中，往往存在着与服务器相比资源相对有限的客户。

分离式学习[2，3]通过将模型架构分割成不同的层，使每个客户保持权重，直到一个被称为分离层的中间层，从而迎合了这种现实的设置。其余的层都在服务器上保存。

**优点和局限性**。 这种方法不仅减少了任何客户端要进行的计算工作，而且还减少了分布式训练期间需要发送的通信有效载荷的大小。这是因为它只需要在前向传播步骤中从任何客户端向服务器发送来自一个层（分裂层）的激活信息。同时，在反向传播步骤中，只有一个层（分裂层之后的层）的梯度需要由服务器发送至客户端。在模型性能方面，我们根据经验观察到，SplitNN的收敛速度仍然比联合学习和大批量同步随机梯度下降[4]快得多。也就是说，当在较少的客户上进行训练时，它需要相对较大的整体通信带宽，尽管在有大量客户的情况下，它最终比其他方法低得多。先进的神经网络压缩方法，如[5-7]，可以用来减少通信负荷。
通信带宽也可以通过允许在客户端有更多的层来表示进一步压缩的表征来换取客户端的计算。

在分割学习中共享中间层的激活也与局部并行[8]、特征重放[9]和分割征服量化[10]的分布式学习方法有关。这是与联合学习中的权重共享相对应的。

### 19.1.1 普通的分离学习

在这种方法中，每个客户端训练网络到某一层，即分裂层，并将权重发送到服务器（图19.1）。然后服务器训练网络的其他层。这就完成了前向传播的过程。然后，服务器为最后一层生成梯度，并反向传播错误，直到分裂层。
然后，梯度被传递给客户端。其余的反向传播由客户端完成。这样一直持续到网络训练完成。分割的形状可以是任意的，不一定是垂直的。在这个框架中，也没有明确的原始数据的共享。

#### 19.1.1.1 同步步骤

在每个客户端完成其历时后，下一个排队完成其历时的客户端会收到来自前一个客户端的本地权重（直到分割层的权重）作为其历时的初始化。

#### 19.1.1.2 放宽同步要求

客户端之间的这种额外的通信可以通过像BlindLearning[11]这样的分割学习方法来避免，该方法基于使用一个损失函数，该函数是每个客户端完成的前向传播获得的损失的平均值。同样，通过splitFedv1[12]、splitFedv2[12]和splitFedv3[13]进一步减少了通信和同步要求，这些都是分割学习和联合学习的混合方法。在[14]中提供了一种改善延迟的混合方法。

### 19.2 通信效率

在这一节中，我们描述了我们对分割学习和联合学习这两种分布式学习设置的通信效率的计算。为了分析通信效率，我们考虑了每个客户端为训练和客户端权重同步所传输的数据量，因为影响通信速率的其他因素取决于训练集群的设置，而与分布式学习设置无关。我们使用以下符号来数学地衡量通信效率。

**符号** K=#客户，N=#模型参数，p=总数据集大小，q=分割层大小，η=客户的模型参数（权重）的比例，因此1-η是服务器的参数比例。

在表19.1中，我们显示了每个客户端在一个历时中所需要的通信，以及所有客户端在一个历时中所需要的总通信。由于有K个客户端，当每个客户端的训练数据集的大小相同时在分割学习中，每个客户会有p/K的数据记录。因此，在前向传播过程中，分裂学习中每个客户端传递的激活大小为(p/K)q，在后向传播过程中，每个客户端传递的梯度大小也为(p/K)q。在有客户端权重共享的虚无分裂学习情况下，将权重传递给下一个客户端将涉及ηN的通信。在联合学习中，在上传单个客户端权重和下载平均权重的过程中，权重/梯度的通信都是ηN。
平均值的过程中，权重/梯度的通信量都是N的大小。

### 19.3 延迟

根据客户端和服务器的计算能力限制，计算的延迟需要最小化，同时保持高的通信效率。为此，[14]对香草分割学习、splitFed和[14]中提出的方法的延迟进行了分析比较。他们考虑了以下模型大小的符号

### 19.4 分离学习的拓扑结构

#### 19.4.1 多样化的配置

#### 19.4.2 用ExpertMatcher选择模型

#### 19.4.3 实现细节

### 19.5 分离学习的协作推理

#### 19.5.1 防止协作推理中的重构攻击

##### 19.5.1.1 信道剪枝

##### 19.5.1.2 相关性

##### 19.5.1.3 损失函数

#### 19.5.2 激活共享的差异性隐私

### 19.6 未来工作

### 参考引用