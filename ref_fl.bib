@inproceedings{mcmahan2017communication,
	title={Communication-efficient learning of deep networks from decentralized data},
	author={McMahan, Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and y Arcas, Blaise Aguera},
	booktitle={Artificial intelligence and statistics},
	pages={1273--1282},
	year={2017},
	organization={PMLR}
}

// Chapter 06
@article{albasyoni2020optimal,
	title={Optimal gradient compression for distributed and federated learning},
	author={Albasyoni, Alyazeed and Safaryan, Mher and Condat, Laurent and Richt{\'a}rik, Peter},
	journal={arXiv preprint arXiv:2010.03246},
	year={2020}
}

@article{alistarh2017qsgd,
	title={QSGD: Communication-efficient SGD via gradient quantization and encoding},
	author={Alistarh, Dan and Grubic, Demjan and Li, Jerry and Tomioka, Ryota and Vojnovic, Milan},
	journal={Advances in neural information processing systems},
	volume={30},
	year={2017}
}

@article{basu2020qsparse,
	title={Qsparse-local-SGD: Distributed SGD with quantization, sparsification, and local computations},
	author={Basu, Debraj and Data, Deepesh and Karakus, Can and Diggavi, Suhas N},
	journal={IEEE Journal on Selected Areas in Information Theory},
	volume={1},
	number={1},
	pages={217--226},
	year={2020},
	publisher={IEEE}
}

@article{bottou2018optimization,
	title={Optimization methods for large-scale machine learning},
	author={Bottou, L{\'e}on and Curtis, Frank E and Nocedal, Jorge},
	journal={SIAM review},
	volume={60},
	number={2},
	pages={223--311},
	year={2018},
	publisher={SIAM}
}

@book{boyd2004convex,
	title={Convex optimization},
	author={Boyd, Stephen and Boyd, Stephen P and Vandenberghe, Lieven},
	year={2004},
	publisher={Cambridge university press}
}

@article{boyd2011distributed,
	title={Distributed optimization and statistical learning via the alternating direction method of multipliers},
	author={Boyd, Stephen and Parikh, Neal and Chu, Eric and Peleato, Borja and Eckstein, Jonathan and others},
	journal={Foundations and Trends{\textregistered} in Machine learning},
	volume={3},
	number={1},
	pages={1--122},
	year={2011},
	publisher={Now Publishers, Inc.}
}

@article{cho2020client,
	title={Client selection in federated learning: Convergence analysis and power-of-choice selection strategies},
	author={Cho, Yae Jee and Wang, Jianyu and Joshi, Gauri},
	journal={arXiv preprint arXiv:2010.01243},
	year={2020}
}

@misc{chaudhari2018stochastic,
	title={Stochastic gradient descent performs variational inference, converges to limit cycles for deep networks}, 
	author={Pratik Chaudhari and Stefano Soatto},
	year={2018},
	eprint={1710.11029},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}

@article{cipar2013solving,
	title={Solving the straggler problem with bounded staleness},
	author={Cipar, James and Ho, Qirong and Kim, Jin Kyu and Lee, Seunghak and Ganger, Gregory R and Gibson, Garth and Keeton, Kimberly and P Xing, Eric},
	year={2013},
	publisher={Carnegie Mellon University}
}

@inproceedings{cui2014exploiting,
	title={Exploiting bounded staleness to speed up big data analytics},
	author={Cui, Henggang and Cipar, James and Ho, Qirong and Kim, Jin Kyu and Lee, Seunghak and Kumar, Abhimanu and Wei, Jinliang and Dai, Wei and Ganger, Gregory R and Gibbons, Phillip B and others},
	booktitle={2014 $\{$USENIX$\}$ Annual Technical Conference ($\{$USENIX$\}$$\{$ATC$\}$ 14)},
	pages={37--48},
	year={2014}
}

@article{dean2012large,
	title={Large scale distributed deep networks},
	author={Dean, Jeffrey and Corrado, Greg and Monga, Rajat and Chen, Kai and Devin, Matthieu and Mao, Mark and Ranzato, Marc'aurelio and Senior, Andrew and Tucker, Paul and Yang, Ke and others},
	journal={Advances in neural information processing systems},
	volume={25},
	year={2012}
}

@article{dekel2012optimal,
	title={Optimal Distributed Online Prediction Using Mini-Batches.},
	author={Dekel, Ofer and Gilad-Bachrach, Ran and Shamir, Ohad and Xiao, Lin},
	journal={Journal of Machine Learning Research},
	volume={13},
	number={1},
	year={2012}
}

@misc{dutta2018slow,
	title={Slow and Stale Gradients Can Win the Race: Error-Runtime Trade-offs in Distributed SGD}, 
	author={Sanghamitra Dutta and Gauri Joshi and Soumyadip Ghosh and Parijat Dube and Priya Nagpurkar},
	year={2018},
	eprint={1803.01113},
	archivePrefix={arXiv},
	primaryClass={stat.ML}
}

@article{frankle2018lottery,
	title={The lottery ticket hypothesis: Finding sparse, trainable neural networks},
	author={Frankle, Jonathan and Carbin, Michael},
	journal={arXiv preprint arXiv:1803.03635},
	year={2018}
}

@inproceedings{gupta2016model,
	title={Model accuracy and runtime tradeoff in distributed deep learning: A systematic study},
	author={Gupta, Suyog and Zhang, Wei and Wang, Fei},
	booktitle={2016 IEEE 16th International Conference on Data Mining (ICDM)},
	pages={171--180},
	year={2016},
	organization={IEEE}
}

@inproceedings{han2020adaptive,
	title={Adaptive gradient sparsification for efficient federated learning: An online learning approach},
	author={Han, Pengchao and Wang, Shiqiang and Leung, Kin K},
	booktitle={2020 IEEE 40th international conference on distributed computing systems (ICDCS)},
	pages={300--310},
	year={2020},
	organization={IEEE}
}

@article{han2015deep,
	title={Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding},
	author={Han, Song and Mao, Huizi and Dally, William J},
	journal={arXiv preprint arXiv:1510.00149},
	year={2015}
}

@article{hazan2016introduction,
	title={Introduction to online convex optimization},
	author={Hazan, Elad and others},
	journal={Foundations and Trends{\textregistered} in Optimization},
	volume={2},
	number={3-4},
	pages={157--325},
	year={2016},
	publisher={Now Publishers, Inc.}
}

@article{ho2013more,
	title={More effective distributed ml via a stale synchronous parallel parameter server},
	author={Ho, Qirong and Cipar, James and Cui, Henggang and Lee, Seunghak and Kim, Jin Kyu and Gibbons, Phillip B and Gibson, Garth A and Ganger, Greg and Xing, Eric P},
	journal={Advances in neural information processing systems},
	volume={26},
	year={2013}
}

@article{horvath2020better,
	title={A better alternative to error feedback for communication-efficient distributed learning},
	author={Horv{\'a}th, Samuel and Richt{\'a}rik, Peter},
	journal={arXiv preprint arXiv:2006.11077},
	year={2020}
}

@INPROCEEDINGS{9443523,
	author={Jee Cho, Yae and Gupta, Samarth and Joshi, Gauri and Yağan, Osman},
	booktitle={2020 54th Asilomar Conference on Signals, Systems, and Computers}, 
	title={Bandit-based Communication-Efficient Client Selection Strategies for Federated Learning}, 
	year={2020},
	volume={},
	number={},
	pages={1066-1069},
	doi={10.1109/IEEECONF51394.2020.9443523}}

@article{jiang2022model,
	title={Model pruning enables efficient federated learning on edge devices},
	author={Jiang, Yuang and Wang, Shiqiang and Valls, Victor and Ko, Bong Jun and Lee, Wei-Han and Leung, Kin K and Tassiulas, Leandros},
	journal={IEEE Transactions on Neural Networks and Learning Systems},
	year={2022},
	publisher={IEEE}
}

@inproceedings{karimireddy2020scaffold,
	title={Scaffold: Stochastic controlled averaging for federated learning},
	author={Karimireddy, Sai Praneeth and Kale, Satyen and Mohri, Mehryar and Reddi, Sashank and Stich, Sebastian and Suresh, Ananda Theertha},
	booktitle={International Conference on Machine Learning},
	pages={5132--5143},
	year={2020},
	organization={PMLR}
}

@inproceedings{karimireddy2019error,
	title={Error feedback fixes signsgd and other gradient compression schemes},
	author={Karimireddy, Sai Praneeth and Rebjock, Quentin and Stich, Sebastian and Jaggi, Martin},
	booktitle={International Conference on Machine Learning},
	pages={3252--3261},
	year={2019},
	organization={PMLR}
}

@inproceedings{li2014efficient,
	title={Efficient mini-batch training for stochastic optimization},
	author={Li, Mu and Zhang, Tong and Chen, Yuqiang and Smola, Alexander J},
	booktitle={Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining},
	pages={661--670},
	year={2014}
}

@inproceedings{li2019feddane,
	title={Feddane: A federated newton-type method},
	author={Li, Tian and Sahu, Anit Kumar and Zaheer, Manzil and Sanjabi, Maziar and Talwalkar, Ameet and Smithy, Virginia},
	booktitle={2019 53rd Asilomar Conference on Signals, Systems, and Computers},
	pages={1227--1231},
	year={2019},
	organization={IEEE}
}

@misc{li2020convergence,
	title={On the Convergence of FedAvg on Non-IID Data}, 
	author={Xiang Li and Kaixuan Huang and Wenhao Yang and Shusen Wang and Zhihua Zhang},
	year={2020},
	eprint={1907.02189},
	archivePrefix={arXiv},
	primaryClass={stat.ML}
}

@article{lian2015asynchronous,
	title={Asynchronous parallel stochastic gradient for nonconvex optimization},
	author={Lian, Xiangru and Huang, Yijun and Li, Yuncheng and Liu, Ji},
	journal={Advances in neural information processing systems},
	volume={28},
	year={2015}
}


@InProceedings{pmlr-v80-lian18a,
	title = 	 {Asynchronous Decentralized Parallel Stochastic Gradient Descent},
	author =       {Lian, Xiangru and Zhang, Wei and Zhang, Ce and Liu, Ji},
	booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
	pages = 	 {3043--3052},
	year = 	 {2018},
	editor = 	 {Dy, Jennifer and Krause, Andreas},
	volume = 	 {80},
	series = 	 {Proceedings of Machine Learning Research},
	month = 	 {10--15 Jul},
	publisher =    {PMLR},
	pdf = 	 {http://proceedings.mlr.press/v80/lian18a/lian18a.pdf},
	url = 	 {https://proceedings.mlr.press/v80/lian18a.html},
	abstract = 	 {Most commonly used distributed machine learning systems are either synchronous or centralized asynchronous. Synchronous algorithms like AllReduce-SGD perform poorly in a heterogeneous environment, while asynchronous algorithms using a parameter server suffer from 1) communication bottleneck at parameter servers when workers are many, and 2) significantly worse convergence when the traffic to parameter server is congested. Can we design an algorithm that is robust in a heterogeneous environment, while being communication efficient and maintaining the best-possible convergence rate? In this paper, we propose an asynchronous decentralized stochastic gradient decent algorithm (AD-PSGD) satisfying all above expectations. Our theoretical analysis shows AD-PSGD converges at the optimal $O(1/\sqrt{K})$ rate as SGD and has linear speedup w.r.t. number of workers. Empirically, AD-PSGD outperforms the best of decentralized parallel SGD (D-PSGD), asynchronous parallel SGD (A-PSGD), and standard data parallel SGD (AllReduce-SGD), often by orders of magnitude in a heterogeneous environment. When training ResNet-50 on ImageNet with up to 128 GPUs, AD-PSGD converges (w.r.t epochs) similarly to the AllReduce-SGD, but each epoch can be up to 4-8x faster than its synchronous counterparts in a network-sharing HPC environment. To the best of our knowledge, AD-PSGD is the first asynchronous algorithm that achieves a similar epoch-wise convergence rate as AllReduce-SGD, at an over 100-GPU scale.}
}

@misc{mcmahan2023communicationefficient,
	title={Communication-Efficient Learning of Deep Networks from Decentralized Data}, 
	author={H. Brendan McMahan and Eider Moore and Daniel Ramage and Seth Hampson and Blaise Agüera y Arcas},
	year={2023},
	eprint={1602.05629},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}

@misc{neyshabur2017geometry,
	title={Geometry of Optimization and Implicit Regularization in Deep Learning}, 
	author={Behnam Neyshabur and Ryota Tomioka and Ruslan Salakhutdinov and Nathan Srebro},
	year={2017},
	eprint={1705.03071},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}

@article{parikh2014proximal,
	title={Proximal algorithms},
	author={Parikh, Neal and Boyd, Stephen and others},
	journal={Foundations and trends{\textregistered} in Optimization},
	volume={1},
	number={3},
	pages={127--239},
	year={2014},
	publisher={Now Publishers, Inc.}
}

@article{recht2011hogwild,
	title={Hogwild!: A lock-free approach to parallelizing stochastic gradient descent},
	author={Recht, Benjamin and Re, Christopher and Wright, Stephen and Niu, Feng},
	journal={Advances in neural information processing systems},
	volume={24},
	year={2011}
}

@inproceedings{reisizadeh2020fedpaq,
	title={Fedpaq: A communication-efficient federated learning method with periodic averaging and quantization},
	author={Reisizadeh, Amirhossein and Mokhtari, Aryan and Hassani, Hamed and Jadbabaie, Ali and Pedarsani, Ramtin},
	booktitle={International Conference on Artificial Intelligence and Statistics},
	pages={2021--2031},
	year={2020},
	organization={PMLR}
}

@article{robbins1951stochastic,
	title={A stochastic approximation method},
	author={Robbins, Herbert and Monro, Sutton},
	journal={The annals of mathematical statistics},
	pages={400--407},
	year={1951},
	publisher={JSTOR}
}

@article{ruder2016overview,
	title={An overview of gradient descent optimization algorithms},
	author={Ruder, Sebastian},
	journal={arXiv preprint arXiv:1609.04747},
	year={2016}
}

@article{russakovsky2015imagenet,
	title={Imagenet large scale visual recognition challenge},
	author={Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and others},
	journal={International journal of computer vision},
	volume={115},
	pages={211--252},
	year={2015},
	publisher={Springer}
}

@article{li2020federated,
	title={Federated optimization in heterogeneous networks},
	author={Li, Tian and Sahu, Anit Kumar and Zaheer, Manzil and Sanjabi, Maziar and Talwalkar, Ameet and Smith, Virginia},
	journal={Proceedings of Machine learning and systems},
	volume={2},
	pages={429--450},
	year={2020}
}

@InProceedings{pmlr-v130-ruan21a,
	title = 	 { Towards Flexible Device Participation in Federated Learning },
	author =       {Ruan, Yichen and Zhang, Xiaoxi and Liang, Shu-Che and Joe-Wong, Carlee},
	booktitle = 	 {Proceedings of The 24th International Conference on Artificial Intelligence and Statistics},
	pages = 	 {3403--3411},
	year = 	 {2021},
	editor = 	 {Banerjee, Arindam and Fukumizu, Kenji},
	volume = 	 {130},
	series = 	 {Proceedings of Machine Learning Research},
	month = 	 {13--15 Apr},
	publisher =    {PMLR},
	pdf = 	 {http://proceedings.mlr.press/v130/ruan21a/ruan21a.pdf},
	url = 	 {https://proceedings.mlr.press/v130/ruan21a.html},
	abstract = 	 { Traditional federated learning algorithms impose strict requirements on the participation rates of devices, which limit the potential reach of federated learning. This paper extends the current learning paradigm to include devices that may become inactive, compute incomplete updates, and depart or arrive in the middle of training. We derive analytical results to illustrate how allowing more flexible device participation can affect the learning convergence when data is not independently and identically distributed (non-IID). We then propose a new federated aggregation scheme that converges even when devices may be inactive or return incomplete updates. We also study how the learning process can adapt to early departures or late arrivals, and analyze their impacts on the convergence. }
}

@misc{li2020federated,
	title={Federated Optimization in Heterogeneous Networks}, 
	author={Tian Li and Anit Kumar Sahu and Manzil Zaheer and Maziar Sanjabi and Ameet Talwalkar and Virginia Smith},
	year={2020},
	eprint={1812.06127},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}

@book{shalev2014understanding,
	title={Understanding machine learning: From theory to algorithms},
	author={Shalev-Shwartz, Shai and Ben-David, Shai},
	year={2014},
	publisher={Cambridge university press}
}

@misc{shwartzziv2017opening,
	title={Opening the Black Box of Deep Neural Networks via Information}, 
	author={Ravid Shwartz-Ziv and Naftali Tishby},
	year={2017},
	eprint={1703.00810},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}

@article{stich2018local,
	title={Local SGD converges fast and communicates little},
	author={Stich, Sebastian U},
	journal={arXiv preprint arXiv:1805.09767},
	year={2018}
}

@article{stich2018sparsified,
	title={Sparsified SGD with memory},
	author={Stich, Sebastian U and Cordonnier, Jean-Baptiste and Jaggi, Martin},
	journal={Advances in Neural Information Processing Systems},
	volume={31},
	year={2018}
}

@inproceedings{tang2019doublesqueeze,
	title={Doublesqueeze: Parallel stochastic gradient descent with double-pass error-compensated compression},
	author={Tang, Hanlin and Yu, Chen and Lian, Xiangru and Zhang, Tong and Liu, Ji},
	booktitle={International Conference on Machine Learning},
	pages={6155--6165},
	year={2019},
	organization={PMLR}
}

@misc{wang2019cooperative,
	title={Cooperative SGD: A unified Framework for the Design and Analysis of Communication-Efficient SGD Algorithms}, 
	author={Jianyu Wang and Gauri Joshi},
	year={2019},
	eprint={1808.07576},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}

@misc{wang2019adaptive,
	title={Adaptive Communication Strategies to Achieve the Best Error-Runtime Trade-off in Local-Update SGD}, 
	author={Jianyu Wang and Gauri Joshi},
	year={2019},
	eprint={1810.08313},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}

@inproceedings{wang2020overlap,
	title={Overlap local-SGD: An algorithmic approach to hide communication delays in distributed SGD},
	author={Wang, Jianyu and Liang, Hao and Joshi, Gauri},
	booktitle={ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
	pages={8871--8875},
	year={2020},
	organization={IEEE}
}

@misc{wang2020tackling,
	title={Tackling the Objective Inconsistency Problem in Heterogeneous Federated Optimization}, 
	author={Jianyu Wang and Qinghua Liu and Hao Liang and Gauri Joshi and H. Vincent Poor},
	year={2020},
	eprint={2007.07481},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}

@inproceedings{
	Wang2020SlowMo:,
	title={SlowMo: Improving Communication-Efficient Distributed SGD with Slow Momentum},
	author={Jianyu Wang and Vinayak Tantia and Nicolas Ballas and Michael Rabbat},
	booktitle={International Conference on Learning Representations},
	year={2020},
	url={https://openreview.net/forum?id=SkxJ8REYPH}
}

@article{wang2021local,
	title={Local adaptivity in federated learning: Convergence and consistency},
	author={Wang, Jianyu and Xu, Zheng and Garrett, Zachary and Charles, Zachary and Liu, Luyang and Joshi, Gauri},
	journal={arXiv preprint arXiv:2106.02305},
	year={2021}
}

@article{wang2019adaptive,
	title={Adaptive federated learning in resource constrained edge computing systems},
	author={Wang, Shiqiang and Tuor, Tiffany and Salonidis, Theodoros and Leung, Kin K and Makaya, Christian and He, Ting and Chan, Kevin},
	journal={IEEE journal on selected areas in communications},
	volume={37},
	number={6},
	pages={1205--1221},
	year={2019},
	publisher={IEEE}
}


@InProceedings{pmlr-v84-yin18a,
	title = 	 {Gradient Diversity: a Key Ingredient for Scalable Distributed Learning},
	author = 	 {Yin, Dong and Pananjady, Ashwin and Lam, Max and Papailiopoulos, Dimitris and Ramchandran, Kannan and Bartlett, Peter},
	booktitle = 	 {Proceedings of the Twenty-First International Conference on Artificial Intelligence and Statistics},
	pages = 	 {1998--2007},
	year = 	 {2018},
	editor = 	 {Storkey, Amos and Perez-Cruz, Fernando},
	volume = 	 {84},
	series = 	 {Proceedings of Machine Learning Research},
	month = 	 {09--11 Apr},
	publisher =    {PMLR},
	pdf = 	 {http://proceedings.mlr.press/v84/yin18a/yin18a.pdf},
	url = 	 {https://proceedings.mlr.press/v84/yin18a.html},
	abstract = 	 {It has been experimentally observed that distributed implementations of mini-batch stochastic gradient descent (SGD) algorithms exhibit speedup saturation and decaying generalization ability beyond a particular batch-size. In this work, we present an analysis hinting that high similarity between concurrently processed gradients may be a cause of this performance degradation. We introduce the notion of gradient diversity that measures the dissimilarity between concurrent gradient updates, and show its key role in the convergence and generalization performance of mini-batch SGD.  We also establish that heuristics similar to DropConnect, Langevin dynamics, and quantization, are provably diversity-inducing mechanisms, and provide experimental evidence indicating that these mechanisms can indeed enable the use of larger batches without sacrificing accuracy and lead to faster training in distributed learning. For example, in one of our experiments, for a convolutional neural network to reach 95% training accuracy on MNIST, using the diversity-inducing mechanism can reduce the training time by 30% in the distributed setting.}
}

@inproceedings{yu2019parallel,
	title={Parallel restarted SGD with faster convergence and less communication: Demystifying why model averaging works for deep learning},
	author={Yu, Hao and Yang, Sen and Zhu, Shenghuo},
	booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
	volume={33},
	number={01},
	pages={5693--5700},
	year={2019}
}

@article{zhang2021understanding,
	title={Understanding deep learning (still) requires rethinking generalization},
	author={Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
	journal={Communications of the ACM},
	volume={64},
	number={3},
	pages={107--115},
	year={2021},
	publisher={ACM New York, NY, USA}
}

@article{zhang2017yellowfin,
	title={Yellowfin and the art of momentum tuning},
	author={Zhang, Jian and Mitliagkas, Ioannis},
	journal={arXiv preprint arXiv:1706.03471},
	year={2017}
}

@article{zhang2015deep,
	title={Deep learning with elastic averaging SGD},
	author={Zhang, Sixin and Choromanska, Anna E and LeCun, Yann},
	journal={Advances in neural information processing systems},
	volume={28},
	year={2015}
}

@article{zhang2015staleness,
	title={Staleness-aware async-sgd for distributed deep learning},
	author={Zhang, Wei and Gupta, Suyog and Lian, Xiangru and Liu, Ji},
	journal={arXiv preprint arXiv:1511.05950},
	year={2015}
}