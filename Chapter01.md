# 第1章 联邦学习介绍

## 摘要

联邦学习（Federated Learning, FL）是一种机器学习的方法，其训练数据不是集中管理的。数据由参与联邦学习过程的数据方保留，不与任何其他实体共享。这使得联邦学习成为一种越来越流行的机器学习任务的解决方案。对于这些任务来说，无论是出于隐私、监管还是实际的原因，将数据集中到一个集中的存储库是有问题的。在本章中，我们介绍了联邦学习的基本概念，概述了它的应用案例，并从机器学习、分布式计算和隐私的角度讨论了它。我们还提供了一个介绍，以深入探讨后续章节中所涉及的事项。

## 1.1 概述

机器学习（Machine Learning, ML）已经成为开发认知和分析功能的关键技术，而这些功能在算法上很难得到有效的开发。随着深度神经网络（Deep Neural Networks, DNN）和能有效训练复杂网络的计算硬件的出现，计算机视觉、语音识别和自然语言理解方面的应用取得了飞跃性的进展。此外，经典的机器学习技术，如决策树、线性回归和支持向量模型（SVMs）也得到了更多的应用，特别是与结构化数据有关的应用。

机器学习的应用在很大程度上取决于高质量训练数据的可用性。但有时，隐私方面的考虑使训练数据无法被带到一个中央数据存储库中，为机器学习过程进行策划和管理。联邦学习（FL）是在[28]中首次以这个名字提出的一种方法，在不同地点的训练数据上训练ML模型，不需要集中收集数据。

这种不愿意使用中央数据存储库的一个重要驱动因素是不同司法管辖区的消费者隐私法规。欧盟的《一般数据保护条例》（GDPR）[50]、《健康保险可携性和责任法案》（HIPAA）[53]和《加州消费者隐私法案》（CCPA）[48]是收集和使用消费者数据的监管框架的范例。此外，关于数据泄露的新闻报道提高了人们对存储敏感消费者数据所带来的责任的认识[9, 42, 43, 51]。联邦学习为使用数据提供了便利，而实际上不需要将其存储在一个中央存储库中，从而减轻了这种风险。监管也限制了数据在不同国家等管辖区之间的流动。这是因为考虑到其他国家的数据保护可能不足或与国家安全有关，要求关键数据保留在岸上[40]。国家和地区的法规对在不同市场拥有子公司但希望使用其所有数据来训练模型的国际公司构成了挑战。除了监管要求，从不同地点的数据中学习也可能只是实用。糟糕的通信连接和由传感器或电信设备收集的大量数据会使中央数据收集不可行。联邦学习也使不同的公司能够在不泄露其商业秘密的情况下，共同创建互利的模型。

那么联邦学习是如何工作的呢？在联邦学习方法中，一组控制着各自训练数据的不同各方，合作训练一个机器学习模型。他们这样做并不与其他各方或任何其他第三方实体分享他们的训练数据。合作的各方在文献中也被称为客户端或设备。当事人可以是各种各样的东西，包括消费者设备，如智能手机或汽车，但也包括不同供应商的云服务，在不同国家处理企业数据的数据中心，公司内部的应用仓，或嵌入式系统，如汽车厂的制造机器人。

虽然联邦学习协作可以以不同的方式进行，但其最常见的形式概述于图1.1。在这种方法中，一个聚合器，有时被称为服务器或协调器，促进了合作。各方根据他们的私人训练数据进行本地训练。当他们的本地训练完成后，他们将他们的模型参数作为模型更新发送到聚合器。模型更新的类型取决于要训练的机器学习模型的类型；例如，对于一个神经网络，模型更新可能是网络的权重。一旦聚合器收到各方的模型更新，它们就可以被合并到一个共同的模型中，这个过程我们称之为*模型融合*。在神经网络的例子中，这可以像FedAvg算法[38]中提出的那样，简单地对权重进行平均化。然后，合并后的模型将作为模型更新再次分发给各方，以形成下一轮学习的基础。这个过程可以重复多轮，直到训练过程收敛。聚合器的作用是协调各方的学习过程和信息交流，并执行融合算法，将各方的模型参数合并为一个共同的模型。融合过程的结果是一个基于各方训练数据的模型，而训练数据从不共享。

联邦学习方法似乎与集群上的分布式学习有关[15]，这是大型机器学习任务的一种常见方法。分布式学习使用一个计算节点集群来分担机器学习的计算工作，从而加速学习过程。分布式学习通常使用一个参数服务器来汇总各节点的结果，这与联合模型中并无不同。然而，它在一些重要方面是不同的。在联邦学习中，数据的分布和数量不是集中控制的，如果所有的训练数据都是私有的，可能是未知的。我们不能对各方数据的独立同分布性（IID）做出假设。同样地，一些当事方可能比其他当事方拥有更多的数据，导致当事方之间数据集的不平衡。在分布式学习中，数据被集中管理，并以分片形式分布到不同的节点，中央实体了解数据的随机属性。在设计联邦学习训练算法时，必须考虑到各方数据的不平衡性和非独立同分布性。

相反，在联邦学习中，各方的数量可能会有所不同，这取决于用例。在一家跨国公司的不同数据中心的数据集上训练一个模型，可能有少于10个当事人。这通常被称为企业[35]或跨语境用例[26]。在一个移动电话应用的数据上进行训练，可能会有数以亿计的各方贡献。这通常被称为跨设备用例[26]。在企业用例中，一般来说，在每一轮中考虑所有或大多数当事方的模型更新是很重要的。在设备用例中，每一轮联邦学习只包括全部设备中的一个潜在的大子样本。在企业用例中，联邦学习过程考虑了相关各方的身份，并可以在训练和验证过程中使用这些。在跨设备的使用案例中，当事人的身份通常并不重要，而且一个当事人可能只参与一轮训练。

在设备使用案例中，比起企业场景，考虑到参与者的数量众多，可以假设一些设备的通信故障。手机可能关闭，或者设备可能处于网络覆盖不佳的地区。这可以通过对各方进行抽样调查和设置执行聚合的时间限制，或其他缓解技术来管理。在企业用例中，由于参与者人数较少，个人的贡献是相关的，所以必须仔细管理通信故障。

在本章的其余部分，我们将对联邦学习进行概述。我们在下一节中对所使用的主要概念进行了正式介绍。之后，我们从三个重要的角度讨论联邦学习，每个角度都有一个单独的章节。首先，我们从机器学习的角度讨论联邦学习；然后，我们通过概述威胁和缓解技术来涵盖安全和隐私的角度；最后，我们对联合学习的系统角度进行了概述。这将为本书的其余部分提供一个起点。

## 1.2 概念和术语

像任何机器学习任务一样，联邦学习在训练数据$\mathcal{D}$上训练一个代表预测函数$f$的模型$\mathcal{M}$。\mathcal{M}可以有一个神经网络或任何其他非神经模型的结构。与集中式机器学习不同的是，$\mathcal{D}$被划分在$n$个当事方$P=\left\{P_{1}, P_{2}, \cdots , P_{n}\right\}$，其中每一方$P_{k} \in P$拥有一个私人训练数据集$\mathcal{D}_{k}$。一个联邦学习过程涉及一个聚合器$A$和一组当事人$P$。必须注意的是，$\mathcal{D}_{k}$只能由当事人$P_{k}$访问。换句话说，除了自己的数据集，没有任何一方知道其他的数据集，而$A$对任何数据集都没有了解。

图1.2显示了联邦学习过程是如何在这个抽象层面上进行的。为了训练一个全局机器学习模型$\mathcal{M}$，聚合器和各方执行一个联邦学习算法，该算法以分布式方式在聚合器和各方上执行。主要的算法组件是每一方的本地训练函数$\mathcal{L}$，它在数据集$\mathcal{D}_{k}$上进行本地训练，以及聚合器的融合函数$\mathcal{F}$，它将每一方的$\mathcal{L}$的结果结合成一个新的联合模型。可以有一组本地训练和融合的迭代，我们称之为轮次，使用索引$t$。算法的执行通过在各方和聚合器之间发送消息来协调。整个过程运行如下:

1. 这个过程从聚合器开始。为了训练模型，聚合器使用一个函数$\mathcal{Q}$，该函数将上一轮训练$\mathcal{M}_{t-1}$的模型作为输入，并为当前回合生成一个查询$q_{t}$。当这个过程开始时，$\mathcal{M}_{0}$可能是空的或只是随机的种子。另外，一些联邦学习算法可能包括$\mathcal{Q}$的额外输入，并可能为每一方定制查询，但为了讨论的简单性，在不损失一般性的情况下，我们使用这种更简单的方法。

2. 查询$q_{t}$被发送到各方，并要求提供关于他们各自的本地模型的信息或关于各方数据集的汇总信息。查询的例子包括对神经网络梯度或模型权重的请求，或对决策树计数的请求。

3. 当收到$q_{t}$时，本地训练过程执行本地训练函数$\mathcal{L}$，该函数将查询$q_{t}$和本地数据集$\mathcal{D}_{k}$作为输入，并输出模型更新$r_{k,t}$。通常情况下，查询$q_{t}$包含了一方可以用来初始化本地训练过程的信息。例如，这包括新的共同模型$\mathcal{M}_{t}$的模型权重，以初始化本地训练，或不同模型类型的其他信息。

4. 当$\mathcal{L}$完成后，$r_{k,t}$从$p_{k}$方发回给聚合器$A$，后者收集所有各方的$r_{k,t}$。

5. 当聚合器收到所有预期各方的模型更新$R_{t} = (r_{1,t}, r_{2,t}, \cdots , r_{n,t})$时，它们被应用融合函数$\mathcal{F}$进行处理，该函数将$R_{t}$作为输入并返回$\mathcal{M}_{t}$。

这个过程可以在多轮中执行，并持续到满足终止标准为止，例如，最大的训练轮数$t_{\max}已经过去，最终形成一个全局模型$\mathcal{M} = \mathcal{M}_{t_{\max}}$。所需的训练轮数可以有很大的不同，从Naive Bayes方法的单一模型合并到典型的基于梯度的机器学习算法的多轮训练。

本地训练函数$\mathcal{L}$，融合函数$\mathcal{F}$，和查询生成函数$\mathcal{Q}$通常是一个互补的集合，被设计为共同工作。$\mathcal{L}$与实际数据集交互，进行局部训练，生成模型更新$R_{k,t}$。$R_{t}$的内容是$\mathcal{F}$的输入，因此，必须由$\mathcal{F}$来解释，它根据这个输入创建下一个模型$\mathcal{M}_{t}$。如果需要另一个回合，$\mathcal{Q}$就会创建另一个查询。

在接下来的章节中，我们将详细描述这一过程在训练神经网络、决策树和梯度增强树的情况下是如何发生的。

我们可以为联邦学习的这一基本方法引入不同的变体。在跨设备联邦学习的情况下，各方的数量往往很大，达到数百万。并非所有各方都参与每一轮。在这种情况下，$\mathcal{Q}$不仅决定了查询，而且决定了哪些$P_{s} \sub P$的当事方要包括在下一轮的查询中。党派的选择可以是随机的，基于党派的特点，或基于先前贡献的优点。

另外，对每一方的查询可能是不同的，$\mathcal{F}$需要在创建一个新的模型$\mathcal{M}_{t}$时整合不同查询的结果。

虽然在大多数情况下，具有单一聚合器的方法是最常用和实用的，但也有人提出了其他替代的联邦学习架构。例如，每一方$P_{k}$可能有它自己的、相关的聚合器$A_{k}$，查询其他各方；各方的集合可能在聚合器之间被分割，并且可能发生一个分层的聚合过程。在介绍的其余部分中，我们重点讨论常见的单一聚合器配置。

## 1.3 机器学习的视角

在这一节中，我们从机器学习的角度来看待联邦学习。联邦学习系统方法的选择--比如在查询中发送什么信息--影响着机器学习行为。我们在下面的小节中针对不同的机器学习范式讨论这个问题。

### 1.3.1 深度神经网络

深度神经网络已经变得非常流行，并且可以轻松的迁移至联邦学习。它的基本方法是在每一方进行本地训练，并在聚合器处融合本地训练结果。本地训练$\mathcal{L}$通常相当于在$P_{k}$方对神经网络进行常规的集中训练，并在每一轮$t$中对其参数$w_{k}$进行优化。我们在每一方$P_{k}$进行优化，在该方的训练数据集$\mathcal{D}_{k}$上最小化参数$w_{k}$（神经网络的权重向量）的损失函数$l$。
$$
w^{*}_{k} = arg \min_{w_{k}}\frac{1}{\mathcal{D}_{k}}\sum_{(x_{i}, y_{i}) \in \mathcal{D}_{k}} l(w_{k}; x_{i}, y_{i})
$$
如果使用梯度下降算法，在给定回合$t$的每个纪元$\tau$中，$w_{k}$的更新方式如下:
$$
w^{t, \tau}_{k} := w^{t, \tau - 1}_{k} - \eta_{k}\nabla l(w^{t, \tau - 1}_{k}, X_{k}, Y_{k})。
$$
损失函数$l$是基于本地数据$\mathcal{D}_{k} = (X_{k}, Y_{k}$计算的，可以是任何合适的函数，如常用的平均平方误差（MSE）。这一轮的参数$w^{t,\tau}_{k}$是使用党派特定的学习率$\eta_{k}$更新的。每一轮本地训练都以来自聚合器的新模型更新为种子$w^{t, 0}_{k}$，它为每一轮的本地训练提供了新的起点。

例如，在建立一个联邦学习系统或一个特定的联邦学习项目时，我们可以就党派-地方超参数做出选择。
- 我们应该为党的本地梯度下降算法选择哪种批量大小：一个，即原始的随机梯度下降（SGD）；整个集合；或一个合适的小批量大小？
- 在向聚合器发送模型更新Rk,t之前要运行多少个本地历时？所有各方在每一轮都应该使用相同数量的历时？在每一方中只训练一个历时，可以防止本地模型wk与对方有很大的差异，但会导致更多的网络流量和频繁的聚合活动。运行多个历时，甚至在不同的一方使用不同数量的历时，会造成更大的差异，但可以用来适应各方计算能力的差异和训练数据集的大小。
- 我们为每一方选择多大的学习率ηk？各方数据分布的差异会使不同的学习率变得有利。
- 其他优化算法可能使用不同的局部超参数，如动量或衰变率[27]。
深度神经网络已经变得非常流行，它的基本方法是在每一方进行本地训练，并在聚合器处融合本地训练结果，以相对简单的方式借给联邦学习。本地训练$\mathcal{L}$通常相当于在$P_{k}$方对神经网络进行常规的集中训练，并在每一轮$t$中对其参数$w_{k}$进行优化。

让我们考虑一个简单的联邦SGD的情况，如[38]所述，在这种情况下，与集中式SGD一样，每个新样本都会导致模型变动。聚合器将选择一方$P_{k}$，并向被选择的一方发送一个查询$q_{t, k}<w_{t}>$。$P_{k}$选择下一个训练样本$(x_{i}, y_{i}) \in \mathcal{D}_{k}$，并执行其本地训练$\mathcal{L}$，计算该样本的损失梯度$\nabla l(w_{t}, x_{i}, y_{i})$。我们将把某一方$P_{k}$在特定回合$t$中的梯度称为$\mathcal{D}_{k}$中训练样本的平均梯度。
$$
g_{k, t} := \frac{1}{|\mathcal{D}_{k}|} \sum_{(x_{i}, y_{i}) \in \mathcal{D}_{k}} \nabla l(w_{k}, x_{i}, y_{i})
$$
$P_{k}$将其作为回复$r_{k, t}<g_{t,k}>$返回给聚合器。然后，聚合器根据$P_{k}$的回复和聚合器的学习率，用模型权重计算新的查询内容：
$$
w_{t+1} := w_{t} - \eta_{a}g_{k, t}。
$$

然后，下一轮开始，由聚合器选择另一方来贡献。在这种简单的方式下，它是相当没效率的，因为它引入了通信开销，并且没有利用并发训练的优势。为了使联邦SGD更加有效率，我们可以在每一方进行小批量的训练，每一轮增加每一方的计算量。我们也可以在所有的或$P_{s} \sub P$的子集上同时进行训练，在计算新的模型权重时对各方的回复梯度进行平均化：
$$
w_{t+1} := w_{t} - \eta_{a}\frac{1}{|K|}\sum_{K}g_{k, t}。
$$
虽然这比天真的方法更有效，但它仍然涉及到与聚合器的大量通信和潜在的协调延迟，当批次大小是完整的$\mathcal{D}_{k}$时，每一个纪元至少有一次协调延迟，当我们使用迷你批次时多次协调。

FedAvg，如[28]中提出的，通过利用每一方的独立处理，更加有效。每一方在回复前都会运行多个历时。与其用梯度回复，各方可以在每一方$P_{k}$直接计算一组新的权重$w_{t,k}$，使用一个共同的学习率$\eta$，并以$r_{k,t}<w_{k,t}, n_{k}>$、他们的模型和样本数进行回复。聚合器的融合算法F对每一方的参数进行平均，以每一方的样本数加权，用于下一轮：
$$
w_{t+1} := \sum_{k \in K}\frac{n_{k}}{n}w_{k, t}。
$$
实验表明，这种方法对不同的模型类型表现良好[38]。FedAvg使用了方程（1.2）中列出的大部分变量，但我们可以想象引入其他参数，如梯度下降算法的局部或可变学习率。

进一步的方法可以在这些基本的FL融合和局部训练算法上进行扩展，以适应数据分布、客户选择和隐私要求的不同属性。[32]中的论文提出了一种基于动量的FL方法来加速收敛，其灵感来自集中式ML优化，如[27]。有状态的优化算法，如ADMM一般只适用于合作中的所有各方每次都参与，保留一方的状态[7]。不同的方法，包括[18]和[17]，使ADMM适应实际的FL设置。FedProx[31]引入了一个近似正则化项，以解决非IID用例中各方的数据异质性。其他方法，如[36]，超越了梯度下降法的优化。

对于每个处理数据异质性、模型结构和各方的具体方面的FL方法，我们需要定义一个算法，该算法由$\mathcal{L}$、$\mathcal{F}$以及各方和聚合器之间的交互协议组成，即$q_{k}$和$r_{k}$的格式。在本书的其余部分，我们发现有不同的最先进的方法来处理数据和模型的异质性方面。

### 1.3.2 经典的机器学习模型

经典的机器学习技术也可以应用于联合学习的场景。其中一些技术可以与DNN非常相似地进行处理。其他技术则必须为分散的训练而完全重新考虑。

**线性模型**，包括回归和分类，可以通过类似于调整神经网络训练过程的方式调整训练过程，在联邦学习中进行训练。具有特征向量$x_{i}=(x^{1}_{i}, x^{2}_{i}, \cdots, x^{m}_{i})$的训练数据可用于训练线性回归的预测器，例如，其形状为
$$
y_{i} = w_{1}x^{1}_{i} + w_{2}x^{2}_{i} + \cdots + w_{m}x^{m}_{i} + b。
$$
它预测$y_{i}$for m个线性变量xji和偏差b，需要最小化权重向量$w$和$b$的损失函数。$w$通常比DNN的权重向量小得多。随着数据D在各方之间划分为Dk，我们可以遵循上一节所述的方法。我们在每一方进行训练，使本地训练数据的损失函数l（wk, bk, xi, yi）最小化。与DNN一样，我们可以选择如何将本地模型融合到一个全局模型中。例如，使用FedAvg作为融合函数F，然后我们可以在本地计算新的本地模型权重，即
$$
wk,t+1:= wt− ηk\nabla l(wk,t+1, Xk, Yk)
$$
在权重的梯度上应用针对各方的学习率ηk。所有各方将他们的模型权重发送到聚合器，在那里权重被平均化，由（wt，bt）定义的新模型M被重新分配给各方。我们也可以应用其他的融合方法，如Federated SGD或上一小节中讨论的任何高级方法。由于w较小，这往往比DNN的情况下收敛得更快。其他经典的线性模型，如逻辑回归或线性支持向量机（SVM）[20]，也可以用类似的方法转化为联合学习方法。

**决策树**和更高级的基于树的模型需要一个不同的方法来进行联合学习，而不是像我们讨论到这里的那些具有静态参数结构的模型类型。决策树是一种成熟的分类模型类型，通常用于分类问题[46]。在决策的可解释性对社会很重要的领域，如医疗、金融和其他监管要求展示决策所依据的标准的领域，它尤为重要。虽然DNN和线性模型可以在本地训练，并且本地参数可以在聚合器处合并，但是还没有提出好的融合算法来将独立训练的树模型合并成一棵决策树。

白皮书[35]描述了ID3算法[46]的联合方法，其中树的形成发生在聚合器上，各方的作用是根据他们的本地训练数据，对提议的类别分割做出计数响应。它适用于数字和分类数据。在其集中的原始版本中，ID3决策树计算每个特征的信息增益，将训练数据集分成给定的类别。它选择具有最大信息增益的特征，并计算该特征的值，使其对D进行最佳分割。一个属性通常不能充分地分割D。对于刚刚创建的树的每个分支，我们递归地应用同样的方法。我们通过计算每个子树数据集相对于其余特征的信息增益，询问哪一个下一个特征能够最好地分割每个子树中的数据子集。该算法继续递归地完善分类，直到当一个树节点的所有成员具有相同的类别标签或满足最大深度时停止。

在联合版本中，聚合器的融合函数F计算信息增益并选择下一个特征来增长树。为了获得计算信息增益的输入，聚合器用提议的特征和分割值查询所有各方。各方计算每个提议的子树的成员及其标签，作为其本地训练函数F，并将这些计数作为回复返回给聚合器。聚合器将所有各方提出的每个特征的计数相加，然后继续计算这些汇总计数的信息增益。与集中式版本一样，选择下一个最佳特征，并再次分割子树，如此循环。

在这种方法中，聚合器发挥了突出的作用，并进行了大部分的计算，而各方主要提供与特征和分割值有关的计数。与其他联合学习方法一样，训练数据从未离开任何一方。根据训练数据集的数量和类成员的数量，这可能需要进一步的隐私保护措施，以确保在这种简单的方法中不会有太多的信息被披露。尽管如此，这是一个很好的例子，说明联合学习的方式与DNN和线性模型的方式不同。

**决策树集合方法**通常比单个决策树提供更好的模型性能。随机森林[8]，特别是梯度提升树，如流行的XGBoost[13]被成功地用于不同的应用，也被用于Kaggle比赛，提供了更好的预测精度。联合随机森林算法可以追求与决策树类似的方法，在聚合器中生长单个树，然后使用各方的数据收集。每次添加时都会随机选择一个特征子集，创建集合体的下一棵树，然后再次从各方查询。对于并非所有各方都拥有每个相关数据记录的相同特征集的情况，提出了更复杂的算法，例如，[34]和[20]。这种情况被称为垂直联合学习（更多内容见下一小节），需要用加密技术将每一方的记录与同一实体相匹配。

梯度增强树在预测效果不佳的决策空间区域加入合集，而在随机森林中则是随机加入。为了确定从合集的下一棵树开始，必须对Dk中的所有训练数据样本计算损失函数，这些样本位于各方。与其他基于树的算法一样，树的生长和对集合体的决策是在聚合器中进行的。然而，除此之外，各方还需要在给聚合器的回复中包括梯度和Hessians，以便对合集的下一棵树做出选择。聚合器的融合函数也需要一个量化的近似值，例如，潜在类别中训练数据样本的直方图。联合梯度增强树，就像其集中训练的对应树一样，通常有很好的准确性，并且可能比其他基于树的学习算法的过拟合更少。Ong等人[45]提出的方法使用党派适应性的量化草图来减少信息泄露。其他联合XGBoost的方法使用加密方法和安全的多方计算方法进行交互和损失计算[14, 33]。这需要在相当的模型性能下有更高的训练时间，适合于需要非常严格的隐私的企业场景。[63]中的概述提供了关于联合梯度提升中隐私权衡的有趣讨论，也可以应用于更简单的基于树的模型。

第二章更详细地介绍了训练树状模型的多种算法，包括梯度增强树。

通过这次简短的参观，我们对最流行经典机器学习和神经网络方法进行了概述。我们看到普通机器学习算法的联邦版本可以通过仔细考虑哪些计算在聚合器进行，哪些在各方进行，以及各方和聚合器之间需要什么样的互动来创建。

### 1.3.3 横向、纵向的联合学习和分离学习

到目前为止，在讨论数据在各方之间的分布时，我们一般假设所有各方的训练数据包括每个样本的相同特征，各方拥有与不同样本有关的数据。例如，医院A有一些病人的健康记录和图像；第二家医院B有其他病人的记录，如图1.3所示。

在神经网络的情况下，我们假设每一方都有同等大小和内容的样本。

然而，在某些情况下，各方可能有不同的特征，指的是同一个实体。再以卫生保健为例，初级保健医生可能有与病人长期就诊有关的电子健康记录，而放射科医生则有与病人疾病有关的图像。一个骨科医生可能有病人的手术记录。在寻找骨科手术健康结果的预测因素时，根据所有三方（初级保健、放射科医生和骨科医生）的数据进行预测可能是有益的。在这种情况下，只有一方，即骨科医生，可能有实际的标签：手术的结果。我们称这个数据集为垂直分割的。

图1.4说明了垂直分区，特征在身份密钥中重叠，以匹配双方的记录，例如，政府标识符。由于并非所有的相关特征都存在于任何一方，所以学习不能在每一方独立进行。此外，身份密钥必须被匹配，以了解每一方的特征如何相互补充。为了保护每一方的数据隐私，我们需要一种加密的方法来匹配数据和执行学习过程。Hardy等人提出了一种基于部分同态加密的开创性的早期方法[24]，其他人如Xu等人[62]提出了更有效的变体，减少了通信和计算要求，以至于它在实际企业实践中变得可行。垂直FL将在第18章中详细介绍。在本章后面，我们将更深入地讨论联合学习的安全和隐私问题。

Vepakomma等人[55]以及其他一些人[49]提出了与垂直联合学习有点关系的分裂学习。在分割学习中，DNN在客户和服务器之间进行分割，客户保持DNN的 "上层 "部分，直到分割层，服务器拥有分割层和下面的部分。在其基本形式中，客户端拥有输入数据，服务器拥有标签。当使用SGD作为训练算法时，前向传递从客户端的输入开始，在分割层传播到服务器。反向传播是通过分割层从服务器到客户端进行的。通过这种方法，一方的数据也可以保持隐私，而另一方则拥有模型结构的一部分。分割式学习可以变化为客户端也有标签，最后一个完全连接的层通过第二个分割层在客户端，或者多个客户端有垂直分割的数据，并使用分割层的分区与服务器进行通信。后一种情况可以被看作是垂直联合学习的概括。第19章更深入地讨论了分割学习。

### 1.3.4 模型个性化

模型的个性化是指根据参与FL过程的特定各方的数据分布，对（联邦训练的）全球模型进行调整。虽然参与FL过程使所有各方都能从大量的训练数据中受益，但有时对最终模型进行个性化处理以确保其反映特定各方所拥有的数据是有益的。如果各方对应于个人用户或组织，这一点尤其重要。在一个天真的情况下，个别当事方可以在本地数据上运行额外的本地训练纪元，以结束FL过程。Wang等人提出了一种方法来评估每一方的个性化的好处[56]。

Mansour等人[37]分析了三种不同的个性化方法：用户聚类，在插值数据（全局和局部之间）上进行训练，以及模型插值。第一种方法需要放宽隐私要求或先进的隐私技术，以基于训练数据对用户进行聚类。数据插值是基于创建一个全局数据集。虽然所有方法都有效，但从隐私的角度来看，模型插值的适用性最广。Grimberg等人提出了一种方法，通过确定优化的权重来优化全局模型和局部模型，以达到个性化的目的，并对之前讨论的方法进行了扩展[22]。

虽然个性化的方法仍在不断发展，但这是对FL过程的一个重要补充。第4章和第5章深入讨论了模型的个性化。

## 1.6 摘要和结论

本章提供了一个介绍联合学习。我们讨论的主要动机训练数据,而不是将所有数据放在一起,因为它是在集中的毫升。需要遵守隐私规定,保密的数据,和务实考虑如网络质量是主要的驱动程序。我们介绍了政党的主要概念和聚合器,然后通过我们需要考虑的主要观点FL:机器学习的角度来看,安全和隐私的角度来看,然后是系统的视角。所有这些观点手拉手去设计一个FL系统适合它的任务。

## 参考文献