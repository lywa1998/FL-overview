# 第1章 联邦学习介绍

## 摘要

联邦学习（Federated Learning, FL）是一种机器学习的方法，其训练数据不是集中管理的。数据由参与联邦学习过程的数据方保留，不与任何其他实体共享。这使得联邦学习成为一种越来越流行的机器学习任务的解决方案。对于这些任务来说，无论是出于隐私、监管还是实际的原因，将数据集中到一个集中的存储库是有问题的。在本章中，我们介绍了联邦学习的基本概念，概述了它的应用案例，并从机器学习、分布式计算和隐私的角度讨论了它。我们还提供了一个介绍，以深入探讨后续章节中所涉及的事项。

## 1.1 概述

机器学习（Machine Learning, ML）已经成为开发认知和分析功能的关键技术，而这些功能在算法上很难得到有效的开发。随着深度神经网络（Deep Neural Networks, DNN）和能有效训练复杂网络的计算硬件的出现，计算机视觉、语音识别和自然语言理解方面的应用取得了飞跃性的进展。此外，经典的机器学习技术，如决策树、线性回归和支持向量模型（SVMs）也得到了更多的应用，特别是与结构化数据有关的应用。

机器学习的应用在很大程度上取决于高质量训练数据的可用性。但有时，隐私方面的考虑使训练数据无法被带到一个中央数据存储库中，为机器学习过程进行策划和管理。联邦学习（FL）是在[28]中首次以这个名字提出的一种方法，在不同地点的训练数据上训练ML模型，不需要集中收集数据。

这种不愿意使用中央数据存储库的一个重要驱动因素是不同司法管辖区的消费者隐私法规。欧盟的《一般数据保护条例》（GDPR）[50]、《健康保险可携性和责任法案》（HIPAA）[53]和《加州消费者隐私法案》（CCPA）[48]是收集和使用消费者数据的监管框架的范例。此外，关于数据泄露的新闻报道提高了人们对存储敏感消费者数据所带来的责任的认识[9, 42, 43, 51]。联邦学习为使用数据提供了便利，而实际上不需要将其存储在一个中央存储库中，从而减轻了这种风险。监管也限制了数据在不同国家等管辖区之间的流动。这是因为考虑到其他国家的数据保护可能不足或与国家安全有关，要求关键数据保留在岸上[40]。国家和地区的法规对在不同市场拥有子公司但希望使用其所有数据来训练模型的国际公司构成了挑战。除了监管要求，从不同地点的数据中学习也可能只是实用。糟糕的通信连接和由传感器或电信设备收集的大量数据会使中央数据收集不可行。联邦学习也使不同的公司能够在不泄露其商业秘密的情况下，共同创建互利的模型。

那么联邦学习是如何工作的呢？在联邦学习方法中，一组控制着各自训练数据的不同各方，合作训练一个机器学习模型。他们这样做并不与其他各方或任何其他第三方实体分享他们的训练数据。合作的各方在文献中也被称为客户端或设备。当事人可以是各种各样的东西，包括消费者设备，如智能手机或汽车，但也包括不同供应商的云服务，在不同国家处理企业数据的数据中心，公司内部的应用仓，或嵌入式系统，如汽车厂的制造机器人。

虽然联邦学习协作可以以不同的方式进行，但其最常见的形式概述于图1.1。在这种方法中，一个聚合器，有时被称为服务器或协调器，促进了合作。各方根据他们的私人训练数据进行本地训练。当他们的本地训练完成后，他们将他们的模型参数作为模型更新发送到聚合器。模型更新的类型取决于要训练的机器学习模型的类型；例如，对于一个神经网络，模型更新可能是网络的权重。一旦聚合器收到各方的模型更新，它们就可以被合并到一个共同的模型中，这个过程我们称之为*模型融合*。在神经网络的例子中，这可以像FedAvg算法[38]中提出的那样，简单地对权重进行平均化。然后，合并后的模型将作为模型更新再次分发给各方，以形成下一轮学习的基础。这个过程可以重复多轮，直到训练过程收敛。聚合器的作用是协调各方的学习过程和信息交流，并执行融合算法，将各方的模型参数合并为一个共同的模型。融合过程的结果是一个基于各方训练数据的模型，而训练数据从不共享。

联邦学习方法似乎与集群上的分布式学习有关[15]，这是大型机器学习任务的一种常见方法。分布式学习使用一个计算节点集群来分担机器学习的计算工作，从而加速学习过程。分布式学习通常使用一个参数服务器来汇总各节点的结果，这与联合模型中并无不同。然而，它在一些重要方面是不同的。在联邦学习中，数据的分布和数量不是集中控制的，如果所有的训练数据都是私有的，可能是未知的。我们不能对各方数据的独立同分布性（IID）做出假设。同样地，一些当事方可能比其他当事方拥有更多的数据，导致当事方之间数据集的不平衡。在分布式学习中，数据被集中管理，并以分片形式分布到不同的节点，中央实体了解数据的随机属性。在设计联邦学习训练算法时，必须考虑到各方数据的不平衡性和非独立同分布性。

相反，在联邦学习中，各方的数量可能会有所不同，这取决于用例。在一家跨国公司的不同数据中心的数据集上训练一个模型，可能有少于10个当事人。这通常被称为企业[35]或跨语境用例[26]。在一个移动电话应用的数据上进行训练，可能会有数以亿计的各方贡献。这通常被称为跨设备用例[26]。在企业用例中，一般来说，在每一轮中考虑所有或大多数当事方的模型更新是很重要的。在设备用例中，每一轮联邦学习只包括全部设备中的一个潜在的大子样本。在企业用例中，联邦学习过程考虑了相关各方的身份，并可以在训练和验证过程中使用这些。在跨设备的使用案例中，当事人的身份通常并不重要，而且一个当事人可能只参与一轮训练。

在设备使用案例中，比起企业场景，考虑到参与者的数量众多，可以假设一些设备的通信故障。手机可能关闭，或者设备可能处于网络覆盖不佳的地区。这可以通过对各方进行抽样调查和设置执行聚合的时间限制，或其他缓解技术来管理。在企业用例中，由于参与者人数较少，个人的贡献是相关的，所以必须仔细管理通信故障。

在本章的其余部分，我们将对联邦学习进行概述。我们在下一节中对所使用的主要概念进行了正式介绍。之后，我们从三个重要的角度讨论联邦学习，每个角度都有一个单独的章节。首先，我们从机器学习的角度讨论联邦学习；然后，我们通过概述威胁和缓解技术来涵盖安全和隐私的角度；最后，我们对联合学习的系统角度进行了概述。这将为本书的其余部分提供一个起点。

## 1.2 概念和术语

像任何机器学习任务一样，联邦学习在训练数据$\mathcal{D}$上训练一个代表预测函数$f$的模型$\mathcal{M}$。\mathcal{M}可以有一个神经网络或任何其他非神经模型的结构。与集中式机器学习不同的是，$\mathcal{D}$被划分在$n$个当事方$P=\left\{P_{1}, P_{2}, \cdots , P_{n}\right\}$，其中每一方$P_{k} \in P$拥有一个私人训练数据集$\mathcal{D}_{k}$。一个联邦学习过程涉及一个聚合器$A$和一组当事人$P$。必须注意的是，$\mathcal{D}_{k}$只能由当事人$P_{k}$访问。换句话说，除了自己的数据集，没有任何一方知道其他的数据集，而$A$对任何数据集都没有了解。

图1.2显示了联邦学习过程是如何在这个抽象层面上进行的。为了训练一个全局机器学习模型$\mathcal{M}$，聚合器和各方执行一个联邦学习算法，该算法以分布式方式在聚合器和各方上执行。主要的算法组件是每一方的本地训练函数$\mathcal{L}$，它在数据集$\mathcal{D}_{k}$上进行本地训练，以及聚合器的融合函数$\mathcal{F}$，它将每一方的$\mathcal{L}$的结果结合成一个新的联合模型。可以有一组本地训练和融合的迭代，我们称之为轮次，使用索引$t$。算法的执行通过在各方和聚合器之间发送消息来协调。整个过程运行如下:

1. 这个过程从聚合器开始。为了训练模型，聚合器使用一个函数$\mathcal{Q}$，该函数将上一轮训练$\mathcal{M}_{t-1}$的模型作为输入，并为当前回合生成一个查询$q_{t}$。当这个过程开始时，$\mathcal{M}_{0}$可能是空的或只是随机的种子。另外，一些联邦学习算法可能包括$\mathcal{Q}$的额外输入，并可能为每一方定制查询，但为了讨论的简单性，在不损失一般性的情况下，我们使用这种更简单的方法。

2. 查询$q_{t}$被发送到各方，并要求提供关于他们各自的本地模型的信息或关于各方数据集的汇总信息。查询的例子包括对神经网络梯度或模型权重的请求，或对决策树计数的请求。

3. 当收到$q_{t}$时，本地训练过程执行本地训练函数$\mathcal{L}$，该函数将查询$q_{t}$和本地数据集$\mathcal{D}_{k}$作为输入，并输出模型更新$r_{k,t}$。通常情况下，查询$q_{t}$包含了一方可以用来初始化本地训练过程的信息。例如，这包括新的共同模型$\mathcal{M}_{t}$的模型权重，以初始化本地训练，或不同模型类型的其他信息。

4. 当$\mathcal{L}$完成后，$r_{k,t}$从$p_{k}$方发回给聚合器$A$，后者收集所有各方的$r_{k,t}$。

5. 当聚合器收到所有预期各方的模型更新$R_{t} = (r_{1,t}, r_{2,t}, \cdots , r_{n,t})$时，它们被应用融合函数$\mathcal{F}$进行处理，该函数将$R_{t}$作为输入并返回$\mathcal{M}_{t}$。

这个过程可以在多轮中执行，并持续到满足终止标准为止，例如，最大的训练轮数$t_{\max}已经过去，最终形成一个全局模型$\mathcal{M} = \mathcal{M}_{t_{\max}}$。所需的训练轮数可以有很大的不同，从Naive Bayes方法的单一模型合并到典型的基于梯度的机器学习算法的多轮训练。

本地训练函数$\mathcal{L}$，融合函数$\mathcal{F}$，和查询生成函数$\mathcal{Q}$通常是一个互补的集合，被设计为共同工作。$\mathcal{L}$与实际数据集交互，进行局部训练，生成模型更新$R_{k,t}$。$R_{t}$的内容是$\mathcal{F}$的输入，因此，必须由$\mathcal{F}$来解释，它根据这个输入创建下一个模型$\mathcal{M}_{t}$。如果需要另一个回合，$\mathcal{Q}$就会创建另一个查询。

在接下来的章节中，我们将详细描述这一过程在训练神经网络、决策树和梯度增强树的情况下是如何发生的。

我们可以为联邦学习的这一基本方法引入不同的变体。在跨设备联邦学习的情况下，各方的数量往往很大，达到数百万。并非所有各方都参与每一轮。在这种情况下，$\mathcal{Q}$不仅决定了查询，而且决定了哪些$P_{s} \sub P$的当事方要包括在下一轮的查询中。党派的选择可以是随机的，基于党派的特点，或基于先前贡献的优点。

另外，对每一方的查询可能是不同的，$\mathcal{F}$需要在创建一个新的模型$\mathcal{M}_{t}$时整合不同查询的结果。

虽然在大多数情况下，具有单一聚合器的方法是最常用和实用的，但也有人提出了其他替代的联邦学习架构。例如，每一方$P_{k}$可能有它自己的、相关的聚合器$A_{k}$，查询其他各方；各方的集合可能在聚合器之间被分割，并且可能发生一个分层的聚合过程。在介绍的其余部分中，我们重点讨论常见的单一聚合器配置。

## 1.3 机器学习的视角

在这一节中，我们从机器学习的角度来看待联邦学习。联邦学习系统方法的选择--比如在查询中发送什么信息--影响着机器学习行为。我们在下面的小节中针对不同的机器学习范式讨论这个问题。

### 1.3.1 深度神经网络


